import numpy
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import pickle
import pefile
import sklearn.ensemble as ske
import joblib

from sklearn.model_selection import train_test_split
from sklearn.feature_selection import SelectFromModel
from sklearn.metrics import confusion_matrix, classification_report
from sklearn.tree import DecisionTreeClassifier


def getInfo():
    print(data.head())
    print('#' * 50)
    print(data.tail())
    print('#' * 50)
    print(data.info())
    print("Count of malware(0) and good files(1): ", data["legitimate"].value_counts())


def visualization():
    data["legitimate"].value_counts().plot(kind='pie', autopct="%1.1f%%")
    plt.show()


data = pd.read_csv('data.csv', sep='|')

X = data.drop(['Name', 'md5', 'legitimate'], axis=1).values
y = data['legitimate'].values

extratrees = ske.ExtraTreesClassifier().fit(X, y)
model = SelectFromModel(extratrees, prefit=True)
X_new = model.transform(X)
nbfeatures = X_new.shape[1]

X_train, X_test, y_train, y_test = train_test_split(X_new, y, test_size=0.29, stratify=y)

features = []
index = numpy.argsort(extratrees.feature_importances_)[::-1][:nbfeatures]
for f in range(nbfeatures):
    print("%d. feature %s (%f)" % (f + 1, data.columns[2 + index[f]], extratrees.feature_importances_[index[f]]))
    features.append(data.columns[2 + f])

model = {"DecisionTree": DecisionTreeClassifier(max_depth=10),
         "RandomForest": ske.RandomForestClassifier(n_estimators=50)}

results = {}
for algo in model:
    clf = model[algo]
    clf.fit(X_train, y_train)
    score = clf.score(X_test, y_test)
    print("%s : %s " % (algo, score))
    results[algo] = score

winner = max(results, key=results.get)
print("Using", winner, "for classification, with", len(features), 'features.')

joblib.dump(model[winner], 'classifier/classifier.pkl')
open('classifier/features.pkl', 'wb').write(pickle.dumps(features))

#
#
# data = pd.read_csv("data.csv", sep='|')
# X = data.drop(["Name", "md5", "legitimate"], axis=1).values
# Y = data["legitimate"].values
#
# fsel = ske.ExtraTreesClassifier().fit(X, Y)
# model = SelectFromModel(fsel, prefit=True)
# X_new = model.transform(X)
# nb_features = X_new.shape[1]
#
# X_train, X_test, Y_train, Y_test = train_test_split(X_new, Y, test_size=0.2)
# print(X_test)
# features = []
#
# # Features identified as important
# print('%i features identified as important:' % nb_features)
# indices = np.argsort(fsel.feature_importances_)[::-1][:nb_features]
#
# for f in range(nb_features):
#     print("%d. feature %s (%f)" % (f + 1, data.columns[2+indices[f]], fsel.feature_importances_[indices[f]]))
#     features.append(data.columns[2+f])
#
#
# model = {
#     "DecisionTree" : DecisionTreeClassifier(max_depth=10),
#     "RandomForest" : ske.RandomForestClassifier(n_estimators=50)
# }
#
#
# results = {}
# for algo in model:
#     clf = model[algo]
#     clf.fit(X_train, Y_train)
#     score = clf.score(X_test, Y_test)
#     print("%s : %s " %(algo, score))
#     results[algo] = score


# winner = max(results, key=results.get)
# print("Using", winner, "for classification, with", len(features), 'features.')


# joblib.dump(model[winner],'classifier/classifier.pkl')
# open('classifier/features.pkl', 'wb').write(pickle.dumps(features))


#
# clf = ske.RandomForestClassifier(n_estimators=50)
# clf.fit(X_train, Y_train)
# score = clf.score(X_test, Y_test)
# Y_pred = clf.predict(X_test)

# print(score * 100)
# print(confusion_matrix(Y_test, Y_pred))
# print(classification_report(Y_test, Y_pred))

# # Writing to files
# joblib.dump(clf, 'classifier/classifier.pkl')
# open('classifier/features.pkl', 'wb').write(pickle.dumps(features))


# pickle.dump(clf, open('classifier/features.pkl', 'wb'))

# loaded_model = pickle.load(open('classifier/features.pkl', 'rb'))
# result = loaded_model.score(X_test, Y_test)
# print(result)

# 99.5110467222021
# [[19395    72]
#  [   63  8080]]
